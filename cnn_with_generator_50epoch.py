# -*- coding: utf-8 -*-
"""CNN_with_generator_50Epoch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C9JNG5bBH1TV-Gty1Nn9GGOK82ipsAJ2
"""

from google.colab import drive
drive.mount('/content/drive')

from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img

#IGNORE: testing the generator out
'''
datagen = ImageDataGenerator(
        rotation_range=40,
        width_shift_range=0.2,
        height_shift_range=0.2,
        shear_range=0.2,
        zoom_range=0.2,
        horizontal_flip=True,
        fill_mode='nearest')

img = load_img('/content/drive/My Drive/MLBootcamp/alzheimers_data/test/MildDemented/26 (19).jpg')  # this is a PIL image
'''
'''
x = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)
x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150)

# the .flow() command below generates batches of randomly transformed images
# and saves the results to the `preview/` directory
i = 0
for batch in datagen.flow(x, batch_size=1,
                          save_to_dir='/content/drive/My Drive/MLBootcamp/alzheimers_data/test/MildDemented/', save_prefix='scan', save_format='jpg'):
    i += 1
    if i > 20:
        break  # otherwise the generator would loop indefinitely
'''

TRAIN_DIR = '/content/drive/My Drive/MLBootcamp/alzheimers_data/train/'
TEST_DIR = '/content/drive/My Drive/MLBootcamp/alzheimers_data/test/'
IMAGE_SIZE = 176
CLASSES = ['MildDemented', 'ModerateDemented', 'NonDemented','VeryMildDemented']
batch_size = 32
num_of_train_samples = 5131
num_of_test_samples = 1279

# this is the augmentation configuration we will use for training
train_datagen = ImageDataGenerator(
        rescale=1./255,
        shear_range=0.2,
        zoom_range=0.2,
        horizontal_flip=True)

# this is the augmentation configuration we will use for testing:
# only rescaling
test_datagen = ImageDataGenerator(rescale=1./255)

# this is a generator that will read pictures found in
# subfolders of 'data/train', and indefinitely generate
# batches of augmented image data
train_generator = train_datagen.flow_from_directory(
          TRAIN_DIR,  # this is the target directory
          target_size=(IMAGE_SIZE, IMAGE_SIZE),  # all images will be resized to 176x176
          batch_size=batch_size,
       #   save_to_dir=TRAIN_DIR, uncomment this line if you want to save images to directory (warning: may cause issues if you run multiple times)
          save_prefix='scan', 
          save_format='jpg',
          color_mode='grayscale',
          class_mode='categorical')  # since it's a classification problem, we set mode to categorical

# this is a similar generator, for validation data
test_generator = test_datagen.flow_from_directory(
        TEST_DIR,
        target_size=(IMAGE_SIZE, IMAGE_SIZE),
        batch_size=batch_size,
       # save_to_dir=TEST_DIR, uncomment this line if you want to save images to directory (warning: may cause issues if you run multiple times)
        save_prefix='scan', 
        save_format='jpg',
        color_mode='grayscale',
        class_mode='categorical')

from keras.layers import Dense, Flatten, BatchNormalization, Dropout, Conv2D, MaxPooling2D, LeakyReLU
from keras.models import Sequential
from keras.preprocessing.image import ImageDataGenerator
from keras.regularizers import l2
from keras.optimizers import Adam

def create_cnn(filters=[32], kernels=[3, 3, 5], dropout=0.5, denses=[128], reg=.0001):

    model = Sequential()
    
    for i, fil in enumerate(filters):
        if i == 0:
            model.add(Conv2D(fil, kernels[0], padding='same', kernel_regularizer=l2(reg), input_shape=(IMAGE_SIZE, IMAGE_SIZE, 1)))
        else:
            model.add(Conv2D(fil, kernels[0], padding='same', kernel_regularizer=l2(reg)))
        model.add(LeakyReLU())
        
        for ker in kernels[1:]:
            model.add(BatchNormalization())
            model.add(Conv2D(fil, ker, padding='same', kernel_regularizer=l2(reg)))
            model.add(LeakyReLU())
            
        model.add(MaxPooling2D())
        model.add(BatchNormalization())
        model.add(Dropout(dropout))

    model.add(Flatten())
    
    for den in denses:
        model.add(Dense(den, kernel_regularizer=l2(reg)))
        model.add(LeakyReLU())
        model.add(BatchNormalization())
        model.add(Dropout(dropout))
    
    model.add(Dense(len(CLASSES), activation='softmax'))
    
    model.compile(
        loss="categorical_crossentropy",
        optimizer=Adam(.001),
        metrics=['acc']
    )

    return model

model = create_cnn(filters=[32, 64, 128], kernels=[3, 3, 5], dropout=0.5, denses=[128, 64], reg=.0001)

history = model.fit_generator(
        train_generator,
        steps_per_epoch=num_of_train_samples // batch_size,
        epochs=50,
        validation_data=test_generator,
        validation_steps=num_of_test_samples // batch_size)
model.save_weights('first_try.h5')  # always save your weights after training or during training

from matplotlib import pyplot as plt

h = history.history
plt.figure(1, figsize=(16, 10))

plt.subplot(121)
plt.xlabel('epoch')
plt.ylabel('loss')
plt.plot(h['loss'], label='training')
plt.plot(h['val_loss'], label='test')
plt.legend()

plt.subplot(122)
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.plot(h['acc'], label='training')
plt.plot(h['val_acc'], label='test')
plt.legend()

plt.show()

import numpy as np
import pandas as pd
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix, f1_score

Y_pred = model.predict_generator(test_generator, num_of_test_samples // batch_size+1)
y_pred = np.argmax(Y_pred, axis=1)
print('F1s = %.3f; %.3f' % (f1_score(test_generator.classes, y_pred, average='micro'), f1_score(test_generator.classes, y_pred, average='macro')))

print('Confusion Matrix')
cm = confusion_matrix(test_generator.classes, y_pred)

cm_norm = cm.copy().astype(float)
for i in range(cm_norm.shape[0]):
    cm_norm[i] = cm_norm[i] / cm_norm[i].sum() * 100

plt.figure(1, figsize=(14, 6))

plt.subplot(121)
plt.title('Confusion matrix')
sns.heatmap(pd.DataFrame(cm, index=CLASSES, columns=CLASSES), annot=True, fmt='d', cbar=False)

plt.subplot(122)
plt.title('Normalized confusion matrix')
sns.heatmap(pd.DataFrame(cm_norm, index=CLASSES, columns=CLASSES), annot=True, fmt='.1f', cbar=False)

plt.show()

print('Classification Report')
print(classification_report(test_generator.classes, y_pred, target_names=CLASSES))